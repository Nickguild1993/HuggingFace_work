{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ+VWMVEdotkLcUO3yej0h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nickguild1993/HuggingFace_work/blob/UNIT_1/Unit_1_lesson3_TYPES_OF_MODELS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ENCODER MODELS"
      ],
      "metadata": {
        "id": "XoVFIrzj05Cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENCODERS, HOW DO THEY WORK?\n",
        "\n",
        "https://youtu.be/MUqNwgPjJvQ\n",
        "\n",
        "Encoder MODELS use only an ENCODER of a TRANSFORMER MODEL"
      ],
      "metadata": {
        "id": "uvu7XaGRxqBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having “bi-directional” attention, and are often called auto-encoding models.\n",
        "\n",
        "* The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.\n",
        "\n",
        "* Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering."
      ],
      "metadata": {
        "id": "Vka1u2J50seF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "Text inputs- \"welcome\", \"to\", \"hell\"\n",
        "\n",
        "--> encoder\n",
        "\n",
        "output each value as a numeric representation: [.1 , .2,...] [.3, .1,...] [.2, .3,...] (also called a feature vector)"
      ],
      "metadata": {
        "id": "6YURy2yfxwQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numeric representation of the word \"to\" isn't just that word in isolation, it takes into account each word in the initial sequence (aka, context)\n",
        "\n",
        "left context: \"welcome\"\n",
        "\n",
        "right context: \"hell\"\n",
        "\n",
        "This is done thanks to the self-attention mechanism\n"
      ],
      "metadata": {
        "id": "6R1kKmOEyCJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of ENCODER use case: Masked Language Modeling\n",
        "\n",
        "My ____ is Nick.\n",
        "\n",
        "Encoders, which use BI-DIRECTIONAL (in this case, left AND right of ___ ) CONTEXT, are good at guessing words in the middle of the sequence because they consider the surrounding context.\n",
        "\n",
        "* this requires SEMANTIC UNDERSTANDING in addition to SYNTATIC UNDERSTANDING"
      ],
      "metadata": {
        "id": "Q1HjCXMQzJKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENCODERS are also good for ***sentiment analysis***\n",
        "\n",
        "* this is becuase encoders are good at obtaining an understanding of sequences; and the relationship/interdependence between words.\n",
        "\n",
        "\"Even though I am sad to see them go, I couldn't be more grateful.\" (positive)\n",
        "\n",
        "\"I am sad to see them go, I can't be grateful\" (negative)"
      ],
      "metadata": {
        "id": "VnwuNkZrzyVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VHH350D50bFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DECODER MODELS"
      ],
      "metadata": {
        "id": "T9NUYLDO08JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/d_ixlCubqQw"
      ],
      "metadata": {
        "id": "ycs0lhcr09_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do they work?\n",
        "\n",
        "OFTEN QUITE SIMILAR TO AN ENCODER\n",
        "\n",
        "\n",
        "\n",
        "Text inputs- \"welcome\", \"to\", \"paintown\"\n",
        "\n",
        "--> ***decoder***\n",
        "\n",
        "output each value as a numeric representation: [.1 , .2,...] [.3, .1,...] [.2, .3,...] (also called a feature vector)\n",
        "\n",
        "* this feature tensor is made up of a vector of values for **each** word of the intitial sequence."
      ],
      "metadata": {
        "id": "0jFDxGjy1AuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***DIFFERENCE BETWEEN ENCODERS AND DECODERS***\n",
        "\n",
        "**the word in question (masked value) can only take the previous words (left side) the right side is hidden**\n",
        "\n",
        "FYI - it can be EITHER *left* OR *right* (but tbh, left has gotta be more common, I reckon)\n",
        "\n",
        "* this is called UNI-DIRECTIONAL, as opposed to encoders which use *bi-directional*\n",
        "\n",
        "This is called ***MASKED self-attention***\n",
        "\n",
        "Given the example above, the numeric representation of \"to\" is only influenced by \"WELCOME\" & \"to\" **NOT** \"paintown\""
      ],
      "metadata": {
        "id": "GAm4czdL184G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**typical decoder model** : CAUSAL LANGUAGE MODELING (guessing the next word in a sentence).\n",
        "\n",
        "Decoders, with their *uni-directional* context, are good at generating words in a given context."
      ],
      "metadata": {
        "id": "EA23EZqT3ZmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "Decoder for CLM\n",
        "\n",
        "Start with: *Me*\n",
        "\n",
        "* Model applies a small transformation to that vector so it maps to all the words known by the model (in this case, *Me*)\n",
        " * Model determines that the most probable following word is *name*\n",
        "\n",
        " * this new word is then added to the initial sequence so the decoder model now has *\"My **name**\"* to work with.\n",
        "        This is where the ***\"auto-regressive\"*** aspect comes into play.\n",
        "\n",
        "**AUTO-REGRESSIVE** Models RE-USE their past outputs as INPUTS in the following steps.\n",
        "\n",
        "* Now we (re)cast the that sequence through the decoder, and retrieve the most probable following word. which in this case, is the word **IS**.\n",
        "\n",
        "This operation is repeated until we're happy with the output."
      ],
      "metadata": {
        "id": "ivKc66C137MV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVDtLksN2WxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SEQUENCE TO SEQUENCE MODELS (ENCODERS-DECODERS)\n",
        "\n",
        "https://youtu.be/0_4KEb08xrE"
      ],
      "metadata": {
        "id": "mC1Uy6nqBInE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do they work?\n",
        "\n",
        "Example of popular sequence to sequence model : T5\n",
        "\n",
        "Start with the ENCODER, which takes the text as inputs, puts those words into the model, and the numeric vector representing the word is the output.\n",
        "\n",
        "**this numeric reprsentation holds meaning about the sequence**\n",
        "\n",
        "Those numeric outputs are passed to the **DECODER** model.\n",
        "\n",
        "The **DECODER** also receives a sequence (if we have no sequence, you can pass the start of sequence word (can also be a prompt) **AS AN INPUT TO THE DECODER, NOT** taken from the encoder output)\n",
        "\n",
        "With that information, the decoder attempts to DECODE the sequence, and outputs a word.\n",
        "\n",
        "With the ENCODER output passed to the decoder, we no longer need the encoder.\n",
        "\n",
        "* As DECODERS ARE AUTO-REGRESSIVE, it will loop over the start of the sequene word again, but apply the generated WORD_1 at the end.  The encoder will then run again, with sequence start + WORD_1 to predict WORD_2. Then WORD_2 is added to the loop after WORD_1 to then predict Word_3.\n",
        "\n",
        "* Order of operations: Sequence (e.g. \"Welcome to Austin\") is passed through the encoder, creating a vector representation.\n",
        "\n",
        "* That vector representation is then sent to the decoder for decoding. NOTE THAT THE ENCODER IS ONLY USED ONCE, BUT THE DECODER, WITH ITS **AUTO REGRESSIVE** nature, is run as long as the text needs to be predicted."
      ],
      "metadata": {
        "id": "WYw84ehABNzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEQUENCE TO SEQUENCE tasks examples: many-to-many; translation; summarization\n",
        "\n",
        "- weights are NOT necessarily shared across the ENCODER and DECODER\n",
        "\n",
        "- input distribution is DIFFERENT from the output distribution\n"
      ],
      "metadata": {
        "id": "giIYY8FwfPtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEQUENCE TO SEQUENCE example: translating english sentence into french\n",
        "\n",
        "start with: \"Transformers are powerful\" into the ENCODER\n",
        "\n",
        "ends with: \"Les transformers sont puissants\"\n",
        "\n",
        "- THE SEQUENCE STARTS AS 3 WORDS BUT THE OUTPUT IS 4 WORDS\n",
        "\n",
        "    the output length is INDEPENDENT of the input length in encoder-decoder models\n",
        "    \n"
      ],
      "metadata": {
        "id": "iz6c-gfHfP0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "klO3JvadfP3T"
      }
    }
  ]
}