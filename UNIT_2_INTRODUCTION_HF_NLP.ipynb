{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0Vnzndi91Mo9zhyuJmdJ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nickguild1993/HuggingFace_work/blob/UNIT_2/UNIT_2_INTRODUCTION_HF_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/learn/nlp-course/chapter2/1?fw=pt"
      ],
      "metadata": {
        "id": "uw1q53uBkE3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNbbVCl4j_Q2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Models vs. Traditional Machine Learning Models\n",
        "Transformer models represent a significant advancement over traditional machine learning models, especially in the realm of natural language processing (NLP) and, increasingly, computer vision. Here's a breakdown of their key differences:\n",
        "\n",
        "**Traditional Machine Learning Models**\n",
        "\n",
        "\n",
        "*   Sequential Processing: These models typically process data sequentially, one element at a time.\n",
        "*   Limited Long-Term Dependency: Models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) can capture some long-term dependencies, but they struggle with very long sequences.\n",
        "* Feature Engineering: Often require extensive feature engineering to extract meaningful information from raw data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Transformer Models**\n",
        "*  Parallel Processing: Transformers process all data points simultaneously, allowing for faster training and better handling of long sequences.\n",
        "*  ***Attention Mechanism:*** The core innovation is the attention mechanism, which enables the model to weigh the importance of different parts of the input data, capturing complex relationships between elements.\n",
        "* End-to-End Learning: Transformers excel at learning directly from raw data, reducing the need for extensive feature engineering.\n",
        "* Scalability: They can handle much larger datasets and more complex tasks compared to traditional models.\n",
        "Key differences summarized:\n",
        "\n",
        "In essence, transformer models are better equipped to handle complex patterns and dependencies within data, making them particularly powerful for tasks involving sequences, such as language translation, text summarization, and image generation"
      ],
      "metadata": {
        "id": "2s242TrQkHGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hugging face transformers library was created to host hundreds of transformer models that can be accessed with a single API through which any Transformer model can be loaded, trianed, and saved.  Main features are:\n",
        "\n",
        "* Ease of use: Just need two lines of code to load badass models\n",
        "\n",
        "* Flexibility: At their core, all models are simple `PyTorch nn.Module` or `Tensorflor tf.keras.model` classes and can thus be handled like any other models in those packages.\n",
        "\n",
        "* Simpliticty: Hardly any abstractions are made across the library. The \"All in one file\" is a core concept: A model's forward pass is completely defined in a single file, so the code itself is easily understood and manipulated."
      ],
      "metadata": {
        "id": "zf3vjE8-kG9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EQyiyyJ2kHJp"
      }
    }
  ]
}