{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlBCEcTtZ4K09LolJguTEd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nickguild1993/HuggingFace_work/blob/UNIT_1/Unit1_lesson2_Transformers_background.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzrAD8ZEP6xb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer architecture was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:\n",
        "\n",
        "June 2018: GPT, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results\n",
        "\n",
        "October 2018: BERT, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)\n",
        "\n",
        "February 2019: GPT-2, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns\n",
        "\n",
        "October 2019: DistilBERT, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT’s performance\n",
        "\n",
        "October 2019: BART and T5, two large pretrained models using the same architecture as the original Transformer model (the first to do so)\n",
        "\n",
        "May 2020, GPT-3, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called zero-shot learning)\n",
        "\n",
        "This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:\n",
        "\n",
        "1. GPT-like (also called auto-regressive Transformer models)\n",
        "2. BERT-like (also called auto-encoding Transformer models)\n",
        "3. BART/T5-like (also called sequence-to-sequence Transformer models)"
      ],
      "metadata": {
        "id": "9xMEJmCAOFyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers are language models\n",
        "All the Transformer models mentioned so far (GPT, BERT, BART, T5, etc.) have been trained as language models. This means they have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!\n",
        "\n",
        "This type of model develops a statistical understanding of the language it has been trained on, but it’s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called transfer learning. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.\n",
        "\n",
        "**An example of a task is predicting the next word in a sentence having read the n previous words. This is called causal language modeling because the output depends on the past and present inputs, but not the future ones.**\n",
        "\n",
        "**Another example is masked language modeling, in which the model predicts a masked word in the sentence.**"
      ],
      "metadata": {
        "id": "m8qx4bDtOW8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretraining is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.\n",
        "\n",
        "this is hella inefficient and also produces inferior results (unless you're using a colossal amount of data) vis-a-vis optimizing an already trained model.\n",
        "\n",
        "Fine-tuning, on the other hand, is the **training done after a model has been pretrained**. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task"
      ],
      "metadata": {
        "id": "4xDPdI20OswW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Transformer architecture\n",
        "\n",
        "Encoders, Decoders and encoders-decoders (sequence to sequence transformer)"
      ],
      "metadata": {
        "id": "rr6221gqOv7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer architecture has two components - an ENCODER, which takes in inputs, and a DECODER, which is fed the information from the encoder and creates outputs"
      ],
      "metadata": {
        "id": "9Kfbsy52qW04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use them together (encoder- decoder) or independently, depending on the task\n",
        "\n",
        "Encoders intake text and transform them into numeric representations so they can be parsed.\n",
        "\n",
        "Decoder \"decodes\" the representations from the encoder"
      ],
      "metadata": {
        "id": "IqCUoQFUquJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg"
      ],
      "metadata": {
        "id": "hS7Z5lomvcKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
        "* Decoder-only models: Good for generative tasks such as text generation.\n",
        "* Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization."
      ],
      "metadata": {
        "id": "8r0J0_FqrjdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Attention layers\n",
        "A key feature of Transformer models is that they are built with special layers called ***attention layers***.\n",
        "\n",
        "* We will explore the details of attention layers later in the course; for now, ***all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.***\n",
        "\n",
        "* To put this into context, consider the task of translating text from English to French. Given the input “You like this course”, a translation model will need to also attend to the adjacent word “You” to get the proper translation for the word “like”, because in French the verb “like” is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating “this” the model will also need to pay attention to the word “course”, because “this” translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of “course”. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.\n",
        "\n",
        "*  The same concept applies to any task associated with natural language: a word by itself has a meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the word being studied."
      ],
      "metadata": {
        "id": "c5p1hcmrsDkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original architecture\n",
        "\n",
        "* The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence).\n",
        "\n",
        "* The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
        "\n",
        "* To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). **For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3.**"
      ],
      "metadata": {
        "id": "oqpfMU7buM9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg"
      ],
      "metadata": {
        "id": "Gb_niR4Dvrer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architectures vs. checkpoints**\n",
        "\n",
        "\n",
        "As we dive into Transformer models in this course, you’ll see mentions of architectures and checkpoints as well as models. These terms all have slightly different meanings:\n",
        "\n",
        "* Architecture: This is the skeleton of the model — the definition of each layer and each operation that happens within the model.\n",
        "* Checkpoints: These are the weights that will be loaded in a given architecture.\n",
        "* Model: This is an umbrella term that isn’t as precise as “architecture” or “checkpoint”: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.\n",
        "\n",
        "\n",
        "For example, BERT is an architecture while bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the bert-base-cased model.”"
      ],
      "metadata": {
        "id": "bn6Pk9ckvVmq"
      }
    }
  ]
}